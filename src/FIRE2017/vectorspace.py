# -*- coding: utf-8 -*-
"""VectorSpace.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1woLnQ_41xd0dobetzyWsNp9VeMRIOCXo
"""

!unzip 'drive/MyDrive/COL764/FIRE2017-IRLeD-track-data.zip'

cd 'FIRE2017-IRLeD-track-data/Task_2'

ls

import pandas as pd
import numpy as np
import string
import os
import sys
import re
from collections import Counter, defaultdict
from nltk import word_tokenize, RegexpTokenizer
from nltk import pos_tag
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import itertools
from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
ps = PorterStemmer()

qrels = pd.read_csv("irled-qrel.txt", sep = ' ', header=None)

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')
countvectorizer = CountVectorizer(analyzer= 'word', stop_words= 'english')

def idfvec(filename,flag=0):
    corpus_files = sorted(os.listdir(filename))
    content_list= []
    mapping = defaultdict(lambda:0)
    count = 0
    for corpus_file in corpus_files:
          filepath = os.path.join(filename, corpus_file)
          with open(filepath, 'r') as f:
              doc_text = f.readlines()
              doc_text = ''.join(doc_text)
              if flag!=0:

                text = word_tokenize(doc_text)
                p_list = pos_tag(text)
                if flag==1:
                  n_list = [x[0] for x in p_list if x[1]=='NN']
                elif flag==2:
                  n_list = [x[0]+';'+x[1] for x in p_list if x[1]=='NN' or x[1]=='VB']
                doc_text = ' '.join(n_list)
              
              content_list.append(doc_text)
              mapping[count] = corpus_file[:-4]
          count+=1
    return content_list,mapping

c_list = idfvec('Prior_Cases',flag=0)

tfidf_wm = tfidfvectorizer.fit_transform(c_list[0])
map = c_list[1]

def findall(p, s):

    i = s.find(p)
    while i != -1:
        yield i
        
        i = s.find(p, i+1)

def process_query(doc):
    citation_marker = "[?CITATION?]"
    
    return findall(citation_marker, doc)

def process_markers(s, i, p):
    
    ls1 = [(a.start(), a.end()) for a in list(re.finditer(' ', s[:i]))]
    
    # another hyper-parameter,take max 40 spaces
    index1 = min(400, len(ls1))
    

    

    # find spaces
    ls2 = [(a.start(), a.end()) for a in list(re.finditer(' ', s[i + len(p):]))]
    
    # another hyper-parameter,take max 40 spaces
    index2 = min(399, len(ls2)-1)
    
    
    # only considering spaces for now
    # may be later add logic for full stops and \n chars.
    return s[:i][ls1[-index1][0]:] + s[i+ len(p):][0: ls2[index2][0]]



def process_queries(filename):
    query_files = os.listdir(filename)
    
    case_queries = defaultdict(lambda:[])
    
    for query_file in query_files:
        
        filepath = os.path.join(filename, query_file)
        
        
        
        with open(filepath, 'r', errors = 'ignore') as f:
            doc_text = f.readlines() 
            doc_text = ''.join(doc_text)
        
        citation_marker_indices = list(process_query(doc_text))
        
        print(query_file, len(citation_marker_indices))
        
        for index in citation_marker_indices:
            query_text = process_markers(doc_text, index, "[?CITATION?]")
            
            case_queries[query_file[:-4]].append(query_text)
            
    return case_queries

def not_proc(filename):
    query_files = os.listdir(filename)
    
    case_queries = defaultdict(lambda:[])
    
    for query_file in query_files:
        
        filepath = os.path.join(filename, query_file)
        
        
        
        with open(filepath, 'r', errors = 'ignore') as f:
            doc_text = f.readlines() 
            doc_text = ''.join(doc_text)
            case_queries[query_file[:-4]].append(doc_text)
    return case_queries

queries = process_queries("Current_Cases")
queries2 = not_proc("Current_Cases")

print(queries2['current_case_0001'])

def AP(qrel, rankings, queryname):
    relevant_docs = qrels[qrels[0] == queryname][2].values
    
    
    relevant_docs_retrieved = 0
    
    precision = []
    for i in range(len(rankings)):
        doc = rankings[i]
        docs_retrieved = i + 1
        
        if doc in relevant_docs:
            relevant_docs_retrieved +=1
            
            precision.append(relevant_docs_retrieved/docs_retrieved)
            
    
    if(len(precision) == 0):
        return 0, 0
        
    
    return np.sum(precision)/5, len(precision)

def MRR(qrel,  rankings, queryname):
    relevant_docs = qrels[qrels[0] == queryname][2].values
    
    rec = 0
    for i in range(len(rankings)):
        doc = rankings[i]
        if doc in relevant_docs:
            return 1/(i+1)
    
    return rec

def P_at_10(qrel,  rankings, queryname):
    relevant_docs = qrels[qrels[0] == queryname][2].values
    
    relevant = 0
    for i in range(10):
        doc = rankings[i]
        if doc in relevant_docs:
            relevant +=1
    
    return relevant/10

def rank(queries):
  list_AP = []
  list_MRR = []
  list_pat = []
  for query in queries:
    query_scores_doc = defaultdict(lambda:0)
    l = queries[query]
    for subq in l:
      query_vec = tfidfvectorizer.transform([subq])
      doc_scores = cosine_similarity(tfidf_wm,query_vec)
      for j in range(len(doc_scores)):
        query_scores_doc[j] = query_scores_doc[j]  if query_scores_doc[j] > doc_scores[j] else doc_scores[j]
    
    results = sorted(query_scores_doc.items(), key = lambda x: x[1], reverse = True)
    rankings = []
      
    for result in results:
        index = result[0]
        rankings.append(map[index])
          
    ap = AP(qrels, rankings, query)
    mrr = MRR(qrels, rankings, query)
    pat = P_at_10(qrels, rankings,query)    
    list_AP.append(ap[0])
    list_MRR.append(mrr)
    list_pat.append(pat)

  MAP = np.mean(list_AP)
  nMRR = np.mean(list_MRR)
  MPAT = np.mean(list_pat)
  print(MAP)
  print(nMRR)
  print(MPAT)
  #query_vec = tfidfvectorizer.transform([queries[query]])

def findmap(method = 'method2'):
  if method=='method2':
    rank(queries)
  else:
    rank(queries2)

findmap()

