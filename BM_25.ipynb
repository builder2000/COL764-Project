{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BM-25",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "10fBuwt14B0whWE3BbECKt6VZk9gn2vWb",
      "authorship_tag": "ABX9TyNyDSBRGRoUt73F5booj7Cl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/builder2000/COL764-Project/blob/main/BM_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmaSg6rJ6epv",
        "outputId": "0b358c9b-14f1-4f28-f1d6-ffea0c318397"
      },
      "source": [
        "cd '/content/drive/MyDrive/COL764 Project'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/COL764 Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1nNvyD-6fv2",
        "outputId": "7d6f4954-4640-4dae-eb64-1f30debf2640"
      },
      "source": [
        "ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \u001b[0m\u001b[01;34mAILA_2019_dataset\u001b[0m/            'COL764 Project Proposal.pdf'\n",
            " AILA_2019_dataset.zip          \u001b[01;34mFIRE2017-IRLeD-track-data\u001b[0m/\n",
            " AILA_2019_Overview_Paper.pdf   FIRE2017-IRLeD-track-data.zip\n",
            " BM-25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xPoyI9V6_pc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01227763-c4cd-43ae-eb73-61e2e7052190"
      },
      "source": [
        "!pip install rank_bm25\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "from nltk import word_tokenize, RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from rank_bm25 import BM25Okapi\n",
        "import itertools\n",
        "ps = PorterStemmer()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.7/dist-packages (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rank_bm25) (1.19.5)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgUxAPfG7OJC"
      },
      "source": [
        "qrels = pd.read_csv(\"FIRE2017-IRLeD-track-data/Task_2/irled-qrel.txt\", sep = ' ', header=None)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBVja45B7a17"
      },
      "source": [
        "# is this stopword list the correct one ? can we try other stopword collections or may be custom ?\n",
        "stopword_set = set(stopwords.words('english'))\n",
        "def stopword_removal(word_list):\n",
        "\n",
        "    return [word for word in word_list if word not in stopword_set ]    \n",
        "\n",
        "\n",
        "# do we make the case lower for legal documents?\n",
        "def tokenize_document(doc, lowercase = False, removeStopwords = True, stemming = False, stemmer = 'PorterStemmer',\n",
        "                      tokenizer = 'RegExpTokeinzer'):\n",
        "    \n",
        "    # punctuations are removed with empty string\n",
        "    doc = doc.translate(str.maketrans('', '', string.punctuation))\n",
        "    \n",
        "    # do we convert to lowercase?\n",
        "    if(lowercase):\n",
        "        doc = doc.lower()\n",
        "    \n",
        "    \n",
        "    # variety of tokenizers could be experimented with\n",
        "    if(tokenizer == 'RegExpTokeinzer'):\n",
        "        tokenizer = RegexpTokenizer(r'\\w+')\n",
        "        tokens = tokenizer.tokenize(doc)\n",
        "    \n",
        "    # word tokenizer of nltk - a bit slow since based on NLP techniques\n",
        "    elif(tokenizer == 'Word_tokenizer'):\n",
        "        tokens = word_tokenize(doc)\n",
        "    \n",
        "        \n",
        "    if(removeStopwords == True):\n",
        "        tokens = stopword_removal(tokens)\n",
        "    \n",
        "        \n",
        "    #variety of stemmers could be experimented with\n",
        "    if(stemming):\n",
        "        if(stemmer == 'PorterStemmer'):\n",
        "            tokens = [ps.stem(token) for token in tokens ]\n",
        "        \n",
        "    \n",
        "    # We can also perform lemmatization if we want\n",
        "    # write function for lemmatizer here.\n",
        "    \n",
        "    \n",
        "    \n",
        "    return tokens\n",
        "        \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvORWINy7tQa"
      },
      "source": [
        "# this function is for processing the corpus\n",
        "def process_corpus(filename, lowercase = False, removeStopwords = True, stemming = False, stemmer = 'PorterStemmer',\n",
        "                    tokenizer = 'RegExpTokeinzer'):\n",
        "    corpus_files = sorted(os.listdir(filename))\n",
        "    \n",
        "    list_tokens = []\n",
        "    \n",
        "    indexes_of_doc = defaultdict(lambda:0)\n",
        "    \n",
        "    count = 0\n",
        "    for corpus_file in corpus_files:\n",
        "        filepath = os.path.join(filename, corpus_file)\n",
        "        \n",
        "        indexes_of_doc[count] = corpus_file[:-4]\n",
        "        print(corpus_file)\n",
        "        with open(filepath, 'r') as f:\n",
        "            doc_text = f.readlines()\n",
        "            \n",
        "            \n",
        "            doc_text = ''.join(doc_text)\n",
        "            \n",
        "            # this considers all words\n",
        "            tokens = tokenize_document(doc_text, lowercase = lowercase, removeStopwords = removeStopwords , stemming = stemming, \n",
        "                                       stemmer = stemmer , tokenizer =  tokenizer)\n",
        "            \n",
        "            list_tokens.append(tokens)\n",
        "        \n",
        "        count +=1\n",
        "    \n",
        "    return list_tokens, indexes_of_doc\n",
        "            \n",
        "\n",
        "            \n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6ZOCA6r70L5"
      },
      "source": [
        "list_tokens, indexes_of_doc = process_corpus(\"FIRE2017-IRLeD-track-data/Task_2/Prior_Cases\", lowercase = True, tokenizer='Word_tokenizer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsKKccoT78ou"
      },
      "source": [
        "# find all indices of occurences of a substring\n",
        "def findall(p, s):\n",
        "\n",
        "    i = s.find(p)\n",
        "    while i != -1:\n",
        "        yield i\n",
        "        \n",
        "        i = s.find(p, i+1)\n",
        "\n",
        "def process_query(doc):\n",
        "    citation_marker = \"[?CITATION?]\"\n",
        "    \n",
        "    return findall(citation_marker, doc)\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFDZPDUX82qO"
      },
      "source": [
        "# function to get the query text by selecting region around the marker, how much to take?\n",
        "\n",
        "# feedback: why construct the whole list ls1 and ls2, rather find one by one upto 40 spaces on either sides.\n",
        "# i guess it would be more efficient.\n",
        "def process_markers(s, i, p):\n",
        "    \n",
        "    ls1 = [(a.start(), a.end()) for a in list(re.finditer(' ', s[:i]))]\n",
        "    \n",
        "    # another hyper-parameter,take max 40 spaces\n",
        "    index1 = min(100, len(ls1))\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    # find spaces\n",
        "    ls2 = [(a.start(), a.end()) for a in list(re.finditer(' ', s[i + len(p):]))]\n",
        "    \n",
        "    # another hyper-parameter,take max 40 spaces\n",
        "    index2 = min(99, len(ls2)-1)\n",
        "    \n",
        "    \n",
        "    # only considering spaces for now\n",
        "    # may be later add logic for full stops and \\n chars.\n",
        "    return s[:i][ls1[-index1][0]:] + s[i+ len(p):][0: ls2[index2][0]]\n",
        "\n",
        "\n",
        "def process_queries(filename):\n",
        "    query_files = os.listdir(filename)\n",
        "    \n",
        "    case_queries = defaultdict(lambda:[])\n",
        "    \n",
        "    for query_file in query_files:\n",
        "        \n",
        "        filepath = os.path.join(filename, query_file)\n",
        "        \n",
        "        \n",
        "        \n",
        "        with open(filepath, 'r', errors = 'ignore') as f:\n",
        "            doc_text = f.readlines() \n",
        "            doc_text = ''.join(doc_text)\n",
        "            \n",
        "        citation_marker_indices = list(process_query(doc_text))\n",
        "        \n",
        "        print(query_file, len(citation_marker_indices))\n",
        "        \n",
        "        for index in citation_marker_indices:\n",
        "            query_text = process_markers(doc_text, index, \"[?CITATION?]\")\n",
        "            \n",
        "            case_queries[query_file[:-4]].append(query_text)\n",
        "            \n",
        "    return case_queries\n",
        "        "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VomL-9K5855G"
      },
      "source": [
        "queries = process_queries(\"FIRE2017-IRLeD-track-data/Task_2/Current_Cases\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PCBE-WA89Dn"
      },
      "source": [
        "# returns the rankings of 2000 docs for a query\n",
        "def bm25_query(queryname, bm25, lowercase = False, removeStopwords = True, stemming = False, stemmer = 'PorterStemmer',\n",
        "               tokenizer = 'RegExpTokeinzer'):\n",
        "    \n",
        "    query_scores_doc = defaultdict(lambda:0)\n",
        "    \n",
        "    for query in queries[queryname]:\n",
        "        \n",
        "        # using all terms \n",
        "        query_tokens = tokenize_document(query, lowercase = lowercase, removeStopwords = removeStopwords, stemming = stemming, \n",
        "                                         stemmer = stemmer, tokenizer = tokenizer)\n",
        "        \n",
        "        #print(query_tokens)\n",
        "        \n",
        "        doc_scores = bm25.get_scores(query_tokens)\n",
        "        \n",
        "        # top 100 queries for each doc\n",
        "        indices = np.argsort(doc_scores)[::-1]\n",
        "        \n",
        "        values = [doc_scores[i] for i in indices]\n",
        "        \n",
        "        \n",
        "        for i in range(len(indices)):\n",
        "            index = indices[i]\n",
        "            score = values[i]\n",
        "            \n",
        "            query_scores_doc[index] = query_scores_doc[index]  if query_scores_doc[index] > score else score\n",
        "\n",
        "     \n",
        "    results = sorted(query_scores_doc.items(), key = lambda x: x[1], reverse = True)\n",
        "    \n",
        "    \n",
        "    rankings = []\n",
        "    \n",
        "    for result in results:\n",
        "        index = result[0]\n",
        "        rankings.append(indexes_of_doc[index])\n",
        "        \n",
        "        \n",
        "    \n",
        "    return rankings\n",
        "        \n",
        "#2nd model (filtering out with idf)\n",
        "def bm25_query_idf(queryname, bm25, lowercase = False, removeStopwords = True, stemming = False, stemmer = 'PorterStemmer',\n",
        "               tokenizer = 'RegExpTokeinzer'):\n",
        "    \n",
        "    query_scores_doc = defaultdict(lambda:0)\n",
        "    \n",
        "    for query in queries[queryname]:\n",
        "        \n",
        "        # using all terms \n",
        "        query_tokens = tokenize_document(query, lowercase = lowercase, removeStopwords = removeStopwords, stemming = stemming, \n",
        "                                         stemmer = stemmer, tokenizer = tokenizer)\n",
        "        \n",
        "        \n",
        "        filtered_query = sorted(query_tokens, key = lambda x : bm25.idf[x] if x in bm25.idf else 0, reverse = True)\n",
        "                \n",
        "        \n",
        "        query_tokens = filtered_query[0: len(filtered_query)//2]\n",
        "        \n",
        "        doc_scores = bm25.get_scores(query_tokens)\n",
        "        \n",
        "        # top 100 queries for each doc\n",
        "        indices = np.argsort(doc_scores)[::-1]\n",
        "        \n",
        "        values = [doc_scores[i] for i in indices]\n",
        "        \n",
        "        \n",
        "        for i in range(len(indices)):\n",
        "            index = indices[i]\n",
        "            score = values[i]\n",
        "            \n",
        "            query_scores_doc[index] = query_scores_doc[index]  if query_scores_doc[index] > score else score\n",
        "\n",
        "     \n",
        "    results = sorted(query_scores_doc.items(), key = lambda x: x[1], reverse = True)\n",
        "    \n",
        "    \n",
        "    rankings = []\n",
        "    \n",
        "    for result in results:\n",
        "        index = result[0]\n",
        "        rankings.append(indexes_of_doc[index])\n",
        "        \n",
        "        \n",
        "    \n",
        "    return rankings"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhGNbGT29I0y"
      },
      "source": [
        "def AP(qrel, rankings, queryname):\n",
        "    relevant_docs = qrels[qrels[0] == queryname][2].values\n",
        "    \n",
        "    \n",
        "    relevant_docs_retrieved = 0\n",
        "    \n",
        "    precision = []\n",
        "    for i in range(len(rankings)):\n",
        "        doc = rankings[i]\n",
        "        docs_retrieved = i + 1\n",
        "        \n",
        "        if doc in relevant_docs:\n",
        "            relevant_docs_retrieved +=1\n",
        "            \n",
        "            precision.append(relevant_docs_retrieved/docs_retrieved)\n",
        "            \n",
        "    \n",
        "    if(len(precision) == 0):\n",
        "        return 0, 0\n",
        "        \n",
        "    \n",
        "    return np.sum(precision)/5, len(precision)\n",
        "\n",
        "\n",
        "def MRR(qrel,  rankings, queryname):\n",
        "    relevant_docs = qrels[qrels[0] == queryname][2].values\n",
        "    \n",
        "    rec = 0\n",
        "    for i in range(len(rankings)):\n",
        "        doc = rankings[i]\n",
        "        if doc in relevant_docs:\n",
        "            return 1/(i+1)\n",
        "    \n",
        "    return rec\n",
        "\n",
        "def P_at_10(qrel,  rankings, queryname):\n",
        "    relevant_docs = qrels[qrels[0] == queryname][2].values\n",
        "    \n",
        "    relevant = 0\n",
        "    for i in range(10):\n",
        "        doc = rankings[i]\n",
        "        if doc in relevant_docs:\n",
        "            relevant +=1\n",
        "    \n",
        "    return relevant/10"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv9EuUnX9L8S"
      },
      "source": [
        "def obtain_relevant_docs(queries, bm25, lowercase = False, removeStopwords = True, stemming = False, stemmer = 'PorterStemmer',\n",
        "                      tokenizer = 'RegExpTokeinzer', model = 1):\n",
        "    \n",
        "    querynames = sorted(queries.keys())\n",
        "    \n",
        "    \n",
        "    list_AP = []\n",
        "    list_MRR = []\n",
        "    list_P10 = []\n",
        "    for queryname in querynames:\n",
        "        if(model == 1):\n",
        "            rankings = bm25_query(queryname, bm25, lowercase = lowercase, removeStopwords = removeStopwords, stemming = stemming, stemmer = stemmer,\n",
        "                              tokenizer = tokenizer)\n",
        "        elif(model == 2):\n",
        "            rankings = bm25_query_idf(queryname, bm25, lowercase = lowercase, removeStopwords = removeStopwords, stemming = stemming, stemmer = stemmer,\n",
        "                              tokenizer = tokenizer)\n",
        "        \n",
        "        ap = AP(qrels, rankings, queryname)\n",
        "        \n",
        "        mrr = MRR(qrels, rankings, queryname)\n",
        "        \n",
        "        p10 = P_at_10(qrels, rankings, queryname)\n",
        "        \n",
        "        print(queryname, ap, mrr, p10)\n",
        "        \n",
        "        list_AP.append(ap[0])\n",
        "        list_MRR.append(mrr)\n",
        "        list_P10.append(p10)\n",
        "        \n",
        "    return np.mean(list_AP), np.mean(list_MRR) , np.mean(list_P10)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w0RJQAC9Ohu"
      },
      "source": [
        "def tune_hyperparameters(lowercase = False, removeStopwords = True, stemming = False, stemmer = 'PorterStemmer',\n",
        "                      tokenizer = 'RegExpTokeinzer', model = 1):\n",
        "    k1_list = np.linspace(1, 2, 5)\n",
        "    b_list = np.linspace(0.70, 1, 6)\n",
        "    \n",
        "    parameter_list = [k1_list, b_list]\n",
        "    hyperparameters = list(itertools.product(*parameter_list))\n",
        "    \n",
        "    MAP_list = defaultdict(lambda:0)\n",
        "    for x in hyperparameters:\n",
        "        print(x)\n",
        "        bm25 = BM25Okapi(list_tokens, k1= x[0], b= x[1])\n",
        "        \n",
        "        map, mrr, p10 = obtain_relevant_docs(queries, bm25, lowercase = lowercase, removeStopwords = removeStopwords, stemming = stemming, stemmer = stemmer,\n",
        "                              tokenizer = tokenizer, model = model)\n",
        "        \n",
        "        MAP_list[x] = map\n",
        "        print(\"MAP is:\", map)\n",
        "    return MAP_list\n",
        "        \n",
        "        "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8xwzCdmAPzG"
      },
      "source": [
        "# stopword removal - True , stem = False\n",
        "list_tokens, indexes_of_doc = process_corpus(\"FIRE2017-IRLeD-track-data/Task_2/Prior_Cases\", lowercase = True, tokenizer = 'Word_tokenizer')\n",
        "bm25 = BM25Okapi(list_tokens, k1= 1.75, b= 0.95)\n",
        "map, mrr, p10 = obtain_relevant_docs(queries, bm25, lowercase = True, tokenizer = 'Word_tokenizer', model = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-z3N94eTbpt"
      },
      "source": [
        "map , mrr, p10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4_5nItLBPx1"
      },
      "source": [
        "# stopword removal - False, stem = False\n",
        "list_tokens, indexes_of_doc = process_corpus(\"FIRE2017-IRLeD-track-data/Task_2/Prior_Cases\", lowercase = True, tokenizer = 'Word_tokenizer'\n",
        "                                            ,removeStopwords = False)\n",
        "bm25 = BM25Okapi(list_tokens, k1= 1.75, b= 0.95)\n",
        "map, mrr, p10 = obtain_relevant_docs(queries, bm25, lowercase = True, removeStopwords = False, tokenizer = 'Word_tokenizer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMUR1ltZ-nCh",
        "outputId": "e76f456b-b06c-41d7-eb13-2967822127c3"
      },
      "source": [
        "map , mrr, p10"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.41994798115082055, 0.7772093257134453, 0.24974874371859299)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPi9wI5ATgby"
      },
      "source": [
        "# stopword removal - True , stem = True\n",
        "list_tokens, indexes_of_doc = process_corpus(\"FIRE2017-IRLeD-track-data/Task_2/Prior_Cases\", lowercase = True, stemming = True ,tokenizer = 'Word_tokenizer')\n",
        "bm25 = BM25Okapi(list_tokens, k1= 1.75, b= 0.95)\n",
        "map, mrr, p10 = obtain_relevant_docs(queries, bm25, lowercase = True, tokenizer = 'Word_tokenizer', model = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkLCOjFNTmay"
      },
      "source": [
        "map mrr, p10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_YhI467Tn2U"
      },
      "source": [
        "# stopword removal - False , stem = True\n",
        "list_tokens, indexes_of_doc = process_corpus(\"FIRE2017-IRLeD-track-data/Task_2/Prior_Cases\", lowercase = True, removeStopwords = False, stemming = True, tokenizer = 'Word_tokenizer')\n",
        "bm25 = BM25Okapi(list_tokens, k1= 1.75, b= 0.95)\n",
        "map, mrr, p10 = obtain_relevant_docs(queries, bm25, lowercase = True, removeStopwords = False, stemming = True, tokenizer = 'Word_tokenizer', model = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJXbGtmsTu3D"
      },
      "source": [
        "map, mrr, p10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYK_S4qWGSTU"
      },
      "source": [
        "# IDF Screeing - stopword removal - True , stem = False\n",
        "list_tokens, indexes_of_doc = process_corpus(\"FIRE2017-IRLeD-track-data/Task_2/Prior_Cases\", lowercase = True, tokenizer = 'Word_tokenizer')\n",
        "bm25 = BM25Okapi(list_tokens, k1= 1.75, b= 0.95)\n",
        "map, mrr, p10 = obtain_relevant_docs(queries, bm25, lowercase = True, tokenizer = 'Word_tokenizer', model = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rFrec1MO1px"
      },
      "source": [
        "map, mrr, p10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x9DVayFO9Tq"
      },
      "source": [
        "# IDF Screeing - stopword removal - False , stem = False\n",
        "list_tokens, indexes_of_doc = process_corpus(\"FIRE2017-IRLeD-track-data/Task_2/Prior_Cases\",  removeStopwords = False, lowercase = True, tokenizer = 'Word_tokenizer')\n",
        "bm25 = BM25Okapi(list_tokens, k1= 1.75, b= 0.95)\n",
        "map, mrr, p10 = obtain_relevant_docs(queries, bm25, lowercase = True, removeStopwords = False, tokenizer = 'Word_tokenizer', model = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZnMIrg3PpqL"
      },
      "source": [
        "map, mrr, p10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBf4Xk5lPq43"
      },
      "source": [
        "# IDF Screeing - stopword removal - False , stem = True\n",
        "list_tokens, indexes_of_doc = process_corpus(\"D:\\Courses\\Sem 7 2021-22\\COL764\\COL764 Project\\FIRE2017-IRLeD-track-data\\Task_2\\Prior_Cases\",  removeStopwords = False, stemming = True, lowercase = True, tokenizer = 'Word_tokenizer')\n",
        "bm25 = BM25Okapi(list_tokens, k1= 1.75, b= 0.95)\n",
        "map, mrr, p10 = obtain_relevant_docs(queries, bm25, lowercase = True, removeStopwords = False, stemming = True, tokenizer = 'Word_tokenizer', model = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic34MW90PvWu"
      },
      "source": [
        "map, mrr, p10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7a6Cf85PwN4"
      },
      "source": [
        "#IDF Screeing - stopword removal - True , stem = True\n",
        "list_tokens, indexes_of_doc = process_corpus(\"D:\\Courses\\Sem 7 2021-22\\COL764\\COL764 Project\\FIRE2017-IRLeD-track-data\\Task_2\\Prior_Cases\",  removeStopwords = True, stemming = True, lowercase = True, tokenizer = 'Word_tokenizer')\n",
        "bm25 = BM25Okapi(list_tokens, k1= 1.75, b= 0.95)\n",
        "map, mrr, p10 = obtain_relevant_docs(queries, bm25, lowercase = True, removeStopwords = True, stemming = True, tokenizer = 'Word_tokenizer', model = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv_8yRzcPycX"
      },
      "source": [
        "map, mrr, p10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET2FIHPrPzk_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}