{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BM-25",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "10fBuwt14B0whWE3BbECKt6VZk9gn2vWb",
      "authorship_tag": "ABX9TyNJrCvW6iEqIfPm2ENUUEPw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/builder2000/COL764-Project/blob/main/BM_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmaSg6rJ6epv",
        "outputId": "fb922c5f-f3da-4c34-d88e-aa7998b4c5e0"
      },
      "source": [
        "cd '/content/drive/MyDrive/COL764 Project'"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/COL764 Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1nNvyD-6fv2",
        "outputId": "b4be6db4-a176-4233-abcc-c96e601e7d70"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \u001b[0m\u001b[01;34mAILA_2019_dataset\u001b[0m/            'COL764 Project Proposal.pdf'\n",
            " AILA_2019_dataset.zip          \u001b[01;34mFIRE2017-IRLeD-track-data\u001b[0m/\n",
            " AILA_2019_Overview_Paper.pdf   FIRE2017-IRLeD-track-data.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMmSqsEJ6-h6",
        "outputId": "33213f51-efe7-42fd-8595-965834f9cebe"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mObject_casedocs\u001b[0m/  Query_doc.txt  relevance_judgments_priorcases.txt\n",
            "\u001b[01;34mObject_statutes\u001b[0m/  README.txt     relevance_judgments_statutes.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xPoyI9V6_pc"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "from nltk import word_tokenize, RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from rank_bm25 import BM25Okapi\n",
        "import itertools\n",
        "ps = PorterStemmer()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgUxAPfG7OJC"
      },
      "source": [
        "qrels = pd.read_csv(\"FIRE2017-IRLeD-track-data/Task_2/irled-qrel.txt\", sep = ' ', header=None)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBVja45B7a17"
      },
      "source": [
        "# is this stopword list the correct one ? can we try other stopword collections or may be custom ?\n",
        "stopword_set = set(stopwords.words('english'))\n",
        "def stopword_removal(word_list):\n",
        "\n",
        "    return [word for word in word_list if word not in stopword_set ]    \n",
        "\n",
        "\n",
        "# do we make the case lower for legal documents?\n",
        "def tokenize_document(doc, lowercase = False, removeStopwords = True, stemming = False, stemmer = 'PorterStemmer',\n",
        "                      tokenizer = 'RegExpTokeinzer'):\n",
        "    \n",
        "    # punctuations are removed with empty string\n",
        "    doc = doc.translate(str.maketrans('', '', string.punctuation))\n",
        "    \n",
        "    # do we convert to lowercase?\n",
        "    if(lowercase):\n",
        "        doc = doc.lower()\n",
        "    \n",
        "    \n",
        "    # variety of tokenizers could be experimented with\n",
        "    if(tokenizer == 'RegExpTokeinzer'):\n",
        "        tokenizer = RegexpTokenizer(r'\\w+')\n",
        "        tokens = tokenizer.tokenize(doc)\n",
        "    \n",
        "    # word tokenizer of nltk - a bit slow since based on NLP techniques\n",
        "    elif(tokenizer == 'Word_tokenizer'):\n",
        "        tokens = word_tokenize(doc)\n",
        "    \n",
        "        \n",
        "    if(removeStopwords == True):\n",
        "        tokens = stopword_removal(tokens)\n",
        "    \n",
        "        \n",
        "    #variety of stemmers could be experimented with\n",
        "    if(stemming):\n",
        "        if(stemmer == 'PorterStemmer'):\n",
        "            tokens = [ps.stem(token) for token in tokens ]\n",
        "        \n",
        "    \n",
        "    # We can also perform lemmatization if we want\n",
        "    # write function for lemmatizer here.\n",
        "    \n",
        "    \n",
        "    \n",
        "    return tokens\n",
        "        \n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvORWINy7tQa"
      },
      "source": [
        "# this function is for processing the corpus\n",
        "def process_corpus(filename, lowercase = False, removeStopwords = True, stemming = False, stemmer = 'PorterStemmer',\n",
        "                    tokenizer = 'RegExpTokeinzer'):\n",
        "    corpus_files = sorted(os.listdir(filename))\n",
        "    \n",
        "    list_tokens = []\n",
        "    \n",
        "    indexes_of_doc = defaultdict(lambda:0)\n",
        "    \n",
        "    count = 0\n",
        "    for corpus_file in corpus_files:\n",
        "        filepath = os.path.join(filename, corpus_file)\n",
        "        \n",
        "        indexes_of_doc[count] = corpus_file[:-4]\n",
        "        print(corpus_file)\n",
        "        with open(filepath, 'r') as f:\n",
        "            doc_text = f.readlines()\n",
        "            \n",
        "            \n",
        "            doc_text = ''.join(doc_text)\n",
        "            \n",
        "            # this considers all words\n",
        "            tokens = tokenize_document(doc_text, lowercase = lowercase, removeStopwords = removeStopwords , stemming = stemming, \n",
        "                                       stemmer = stemmer , tokenizer =  tokenizer)\n",
        "            \n",
        "            list_tokens.append(tokens)\n",
        "        \n",
        "        count +=1\n",
        "    \n",
        "    return list_tokens, indexes_of_doc\n",
        "            \n",
        "\n",
        "            \n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6ZOCA6r70L5"
      },
      "source": [
        "list_tokens, indexes_of_doc = process_corpus(\"FIRE2017-IRLeD-track-data/Task_2/Prior_Cases\", lowercase = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsKKccoT78ou"
      },
      "source": [
        "# find all indices of occurences of a substring\n",
        "def findall(p, s):\n",
        "\n",
        "    i = s.find(p)\n",
        "    while i != -1:\n",
        "        yield i\n",
        "        \n",
        "        i = s.find(p, i+1)\n",
        "\n",
        "def process_query(doc):\n",
        "    citation_marker = \"[?CITATION?]\"\n",
        "    \n",
        "    return findall(citation_marker, doc)\n",
        "\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFDZPDUX82qO"
      },
      "source": [
        "# function to get the query text by selecting region around the marker, how much to take?\n",
        "\n",
        "# feedback: why construct the whole list ls1 and ls2, rather find one by one upto 40 spaces on either sides.\n",
        "# i guess it would be more efficient.\n",
        "def process_markers(s, i, p):\n",
        "    \n",
        "    ls1 = [(a.start(), a.end()) for a in list(re.finditer(' ', s[:i]))]\n",
        "    \n",
        "    # another hyper-parameter,take max 40 spaces\n",
        "    index1 = min(40, len(ls1))\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    # find spaces\n",
        "    ls2 = [(a.start(), a.end()) for a in list(re.finditer(' ', s[i + len(p):]))]\n",
        "    \n",
        "    # another hyper-parameter,take max 40 spaces\n",
        "    index2 = min(39, len(ls2)-1)\n",
        "    \n",
        "    \n",
        "    # only considering spaces for now\n",
        "    # may be later add logic for full stops and \\n chars.\n",
        "    return s[:i][ls1[-index1][0]:] + s[i+ len(p):][0: ls2[index2][0]]\n",
        "\n",
        "\n",
        "def process_queries(filename):\n",
        "    query_files = os.listdir(filename)\n",
        "    \n",
        "    case_queries = defaultdict(lambda:[])\n",
        "    \n",
        "    for query_file in query_files:\n",
        "        \n",
        "        filepath = os.path.join(filename, query_file)\n",
        "        \n",
        "        \n",
        "        \n",
        "        with open(filepath, 'r', errors = 'ignore') as f:\n",
        "            doc_text = f.readlines() \n",
        "            doc_text = ''.join(doc_text)\n",
        "            \n",
        "        citation_marker_indices = list(process_query(doc_text))\n",
        "        \n",
        "        print(query_file, len(citation_marker_indices))\n",
        "        \n",
        "        for index in citation_marker_indices:\n",
        "            query_text = process_markers(doc_text, index, \"[?CITATION?]\")\n",
        "            \n",
        "            case_queries[query_file[:-4]].append(query_text)\n",
        "            \n",
        "    return case_queries\n",
        "        "
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VomL-9K5855G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4906456-1da9-4445-a3fa-a0a8ab2187b1"
      },
      "source": [
        "queries = process_queries(\"FIRE2017-IRLeD-track-data/Task_2/Current_Cases\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current_case_0086.txt 17\n",
            "current_case_0191.txt 12\n",
            "current_case_0106.txt 14\n",
            "current_case_0016.txt 10\n",
            "current_case_0031.txt 22\n",
            "current_case_0135.txt 16\n",
            "current_case_0061.txt 19\n",
            "current_case_0046.txt 18\n",
            "current_case_0170.txt 49\n",
            "current_case_0042.txt 12\n",
            "current_case_0152.txt 30\n",
            "current_case_0102.txt 39\n",
            "current_case_0188.txt 37\n",
            "current_case_0034.txt 8\n",
            "current_case_0023.txt 28\n",
            "current_case_0035.txt 10\n",
            "current_case_0129.txt 14\n",
            "current_case_0075.txt 14\n",
            "current_case_0028.txt 23\n",
            "current_case_0146.txt 16\n",
            "current_case_0055.txt 14\n",
            "current_case_0077.txt 13\n",
            "current_case_0199.txt 0\n",
            "current_case_0010.txt 37\n",
            "current_case_0082.txt 10\n",
            "current_case_0002.txt 15\n",
            "current_case_0043.txt 41\n",
            "current_case_0141.txt 25\n",
            "current_case_0030.txt 8\n",
            "current_case_0123.txt 26\n",
            "current_case_0098.txt 25\n",
            "current_case_0174.txt 32\n",
            "current_case_0195.txt 23\n",
            "current_case_0004.txt 32\n",
            "current_case_0103.txt 15\n",
            "current_case_0161.txt 20\n",
            "current_case_0003.txt 19\n",
            "current_case_0144.txt 13\n",
            "current_case_0136.txt 18\n",
            "current_case_0134.txt 6\n",
            "current_case_0138.txt 14\n",
            "current_case_0200.txt 7\n",
            "current_case_0085.txt 21\n",
            "current_case_0021.txt 13\n",
            "current_case_0032.txt 25\n",
            "current_case_0097.txt 20\n",
            "current_case_0090.txt 14\n",
            "current_case_0039.txt 26\n",
            "current_case_0128.txt 16\n",
            "current_case_0158.txt 18\n",
            "current_case_0116.txt 6\n",
            "current_case_0049.txt 12\n",
            "current_case_0015.txt 10\n",
            "current_case_0186.txt 24\n",
            "current_case_0026.txt 17\n",
            "current_case_0024.txt 13\n",
            "current_case_0143.txt 7\n",
            "current_case_0147.txt 19\n",
            "current_case_0114.txt 20\n",
            "current_case_0150.txt 16\n",
            "current_case_0068.txt 8\n",
            "current_case_0194.txt 15\n",
            "current_case_0084.txt 18\n",
            "current_case_0022.txt 29\n",
            "current_case_0121.txt 25\n",
            "current_case_0167.txt 11\n",
            "current_case_0069.txt 16\n",
            "current_case_0108.txt 34\n",
            "current_case_0088.txt 13\n",
            "current_case_0181.txt 36\n",
            "current_case_0178.txt 24\n",
            "current_case_0169.txt 11\n",
            "current_case_0053.txt 22\n",
            "current_case_0092.txt 20\n",
            "current_case_0006.txt 13\n",
            "current_case_0029.txt 22\n",
            "current_case_0065.txt 18\n",
            "current_case_0110.txt 8\n",
            "current_case_0139.txt 9\n",
            "current_case_0074.txt 18\n",
            "current_case_0071.txt 11\n",
            "current_case_0045.txt 62\n",
            "current_case_0115.txt 28\n",
            "current_case_0124.txt 27\n",
            "current_case_0154.txt 26\n",
            "current_case_0056.txt 13\n",
            "current_case_0012.txt 20\n",
            "current_case_0073.txt 7\n",
            "current_case_0168.txt 28\n",
            "current_case_0066.txt 24\n",
            "current_case_0137.txt 13\n",
            "current_case_0159.txt 33\n",
            "current_case_0081.txt 16\n",
            "current_case_0047.txt 25\n",
            "current_case_0160.txt 14\n",
            "current_case_0033.txt 12\n",
            "current_case_0190.txt 10\n",
            "current_case_0176.txt 16\n",
            "current_case_0113.txt 9\n",
            "current_case_0017.txt 77\n",
            "current_case_0091.txt 12\n",
            "current_case_0197.txt 25\n",
            "current_case_0018.txt 16\n",
            "current_case_0125.txt 14\n",
            "current_case_0198.txt 19\n",
            "current_case_0019.txt 19\n",
            "current_case_0087.txt 23\n",
            "current_case_0175.txt 17\n",
            "current_case_0020.txt 10\n",
            "current_case_0131.txt 9\n",
            "current_case_0133.txt 8\n",
            "current_case_0041.txt 16\n",
            "current_case_0149.txt 10\n",
            "current_case_0177.txt 18\n",
            "current_case_0118.txt 19\n",
            "current_case_0067.txt 13\n",
            "current_case_0008.txt 27\n",
            "current_case_0099.txt 12\n",
            "current_case_0142.txt 17\n",
            "current_case_0182.txt 12\n",
            "current_case_0064.txt 11\n",
            "current_case_0164.txt 56\n",
            "current_case_0007.txt 26\n",
            "current_case_0105.txt 12\n",
            "current_case_0165.txt 13\n",
            "current_case_0096.txt 27\n",
            "current_case_0009.txt 13\n",
            "current_case_0107.txt 21\n",
            "current_case_0059.txt 67\n",
            "current_case_0109.txt 35\n",
            "current_case_0166.txt 11\n",
            "current_case_0145.txt 11\n",
            "current_case_0187.txt 27\n",
            "current_case_0025.txt 14\n",
            "current_case_0089.txt 9\n",
            "current_case_0112.txt 26\n",
            "current_case_0072.txt 16\n",
            "current_case_0173.txt 11\n",
            "current_case_0052.txt 9\n",
            "current_case_0192.txt 15\n",
            "current_case_0083.txt 20\n",
            "current_case_0140.txt 14\n",
            "current_case_0111.txt 15\n",
            "current_case_0172.txt 21\n",
            "current_case_0054.txt 16\n",
            "current_case_0127.txt 31\n",
            "current_case_0036.txt 7\n",
            "current_case_0094.txt 9\n",
            "current_case_0184.txt 12\n",
            "current_case_0196.txt 7\n",
            "current_case_0070.txt 8\n",
            "current_case_0157.txt 20\n",
            "current_case_0151.txt 16\n",
            "current_case_0126.txt 9\n",
            "current_case_0130.txt 22\n",
            "current_case_0076.txt 29\n",
            "current_case_0044.txt 31\n",
            "current_case_0119.txt 17\n",
            "current_case_0093.txt 17\n",
            "current_case_0060.txt 5\n",
            "current_case_0179.txt 13\n",
            "current_case_0189.txt 27\n",
            "current_case_0180.txt 10\n",
            "current_case_0050.txt 18\n",
            "current_case_0120.txt 19\n",
            "current_case_0132.txt 7\n",
            "current_case_0163.txt 8\n",
            "current_case_0048.txt 65\n",
            "current_case_0057.txt 20\n",
            "current_case_0040.txt 15\n",
            "current_case_0063.txt 29\n",
            "current_case_0156.txt 11\n",
            "current_case_0104.txt 26\n",
            "current_case_0100.txt 1\n",
            "current_case_0162.txt 13\n",
            "current_case_0148.txt 16\n",
            "current_case_0014.txt 37\n",
            "current_case_0037.txt 37\n",
            "current_case_0122.txt 28\n",
            "current_case_0027.txt 5\n",
            "current_case_0185.txt 11\n",
            "current_case_0193.txt 14\n",
            "current_case_0005.txt 15\n",
            "current_case_0095.txt 7\n",
            "current_case_0155.txt 21\n",
            "current_case_0038.txt 15\n",
            "current_case_0051.txt 60\n",
            "current_case_0013.txt 30\n",
            "current_case_0078.txt 22\n",
            "current_case_0183.txt 20\n",
            "current_case_0062.txt 19\n",
            "current_case_0171.txt 13\n",
            "current_case_0079.txt 19\n",
            "current_case_0117.txt 28\n",
            "current_case_0058.txt 17\n",
            "current_case_0080.txt 15\n",
            "current_case_0153.txt 7\n",
            "current_case_0011.txt 11\n",
            "current_case_0101.txt 16\n",
            "current_case_0001.txt 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PCBE-WA89Dn"
      },
      "source": [
        "# returns the rankings of 2000 docs for a query\n",
        "def bm25_query(queryname, bm25, lowercase = False, removeStopwords = True, stemming = False, stemmer = 'PorterStemmer',\n",
        "               tokenizer = 'RegExpTokeinzer'):\n",
        "    \n",
        "    query_scores_doc = defaultdict(lambda:0)\n",
        "    \n",
        "    for query in queries[queryname]:\n",
        "        \n",
        "        # using all terms \n",
        "        query_tokens = tokenize_document(query, lowercase = lowercase, removeStopwords = removeStopwords, stemming = stemming, \n",
        "                                         stemmer = stemmer, tokenizer = tokenizer)\n",
        "        \n",
        "        #print(query_tokens)\n",
        "        \n",
        "        doc_scores = bm25.get_scores(query_tokens)\n",
        "        \n",
        "        # top 100 queries for each doc\n",
        "        indices = np.argsort(doc_scores)[::-1]\n",
        "        \n",
        "        values = [doc_scores[i] for i in indices]\n",
        "        \n",
        "        \n",
        "        for i in range(len(indices)):\n",
        "            index = indices[i]\n",
        "            score = values[i]\n",
        "            \n",
        "            query_scores_doc[index] = query_scores_doc[index]  if query_scores_doc[index] > score else score\n",
        "\n",
        "     \n",
        "    results = sorted(query_scores_doc.items(), key = lambda x: x[1], reverse = True)\n",
        "    \n",
        "    \n",
        "    rankings = []\n",
        "    \n",
        "    for result in results:\n",
        "        index = result[0]\n",
        "        rankings.append(indexes_of_doc[index])\n",
        "        \n",
        "        \n",
        "    \n",
        "    return rankings\n",
        "        \n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhGNbGT29I0y"
      },
      "source": [
        "def AP(qrel, rankings, queryname):\n",
        "    relevant_docs = qrels[qrels[0] == queryname][2].values\n",
        "    \n",
        "    \n",
        "    relevant_docs_retrieved = 0\n",
        "    \n",
        "    precision = []\n",
        "    for i in range(len(rankings)):\n",
        "        doc = rankings[i]\n",
        "        docs_retrieved = i + 1\n",
        "        \n",
        "        if doc in relevant_docs:\n",
        "            relevant_docs_retrieved +=1\n",
        "            \n",
        "            precision.append(relevant_docs_retrieved/docs_retrieved)\n",
        "            \n",
        "    \n",
        "    if(len(precision) == 0):\n",
        "        return 0, 0\n",
        "        \n",
        "    \n",
        "    return np.sum(precision)/5, len(precision)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv9EuUnX9L8S"
      },
      "source": [
        "def obtain_relevant_docs(queries, bm25, lowercase = False, removeStopwords = True, stemming = False, stemmer = 'PorterStemmer',\n",
        "                      tokenizer = 'RegExpTokeinzer', model = 1):\n",
        "    \n",
        "    querynames = queries.keys()\n",
        "    \n",
        "    \n",
        "    list_AP = []\n",
        "    for queryname in querynames:\n",
        "        if(model == 1):\n",
        "            rankings = bm25_query(queryname, bm25, lowercase = lowercase, removeStopwords = removeStopwords, stemming = stemming, stemmer = stemmer,\n",
        "                              tokenizer = tokenizer)\n",
        "        elif(model == 2):\n",
        "            rankings = bm25_query_idf(queryname, bm25, lowercase = lowercase, removeStopwords = removeStopwords, stemming = stemming, stemmer = stemmer,\n",
        "                              tokenizer = tokenizer)\n",
        "        \n",
        "        ap = AP(qrels, rankings, queryname)\n",
        "        \n",
        "        print(queryname, ap)\n",
        "        \n",
        "        list_AP.append(ap[0])\n",
        "        \n",
        "    return np.mean(list_AP)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w0RJQAC9Ohu"
      },
      "source": [
        "def tune_hyperparameters(lowercase = False, removeStopwords = True, stemming = False, stemmer = 'PorterStemmer',\n",
        "                      tokenizer = 'RegExpTokeinzer', model = 1):\n",
        "    k1_list = np.linspace(1, 2, 5)\n",
        "    b_list = np.linspace(0.70, 1, 6)\n",
        "    \n",
        "    parameter_list = [k1_list, b_list]\n",
        "    hyperparameters = list(itertools.product(*parameter_list))\n",
        "    \n",
        "    MAP_list = defaultdict(lambda:0)\n",
        "    for x in hyperparameters:\n",
        "        print(x)\n",
        "        bm25 = BM25Okapi(list_tokens, k1= x[0], b= x[1])\n",
        "        \n",
        "        map = obtain_relevant_docs(queries, bm25, lowercase = lowercase, removeStopwords = removeStopwords, stemming = stemming, stemmer = stemmer,\n",
        "                              tokenizer = tokenizer, model = model)\n",
        "        \n",
        "        MAP_list[x] = map\n",
        "        print(\"MAP is:\", map)\n",
        "    return MAP_list\n",
        "        \n",
        "        "
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGR4GXJ_9RKs"
      },
      "source": [
        "list_tokens, indexes_of_doc = process_corpus(\"FIRE2017-IRLeD-track-data/Task_2/Prior_Cases\", lowercase = True)\n",
        "bm25 = BM25Okapi(list_tokens, k1= 1.5, b= 0.94)\n",
        "map = obtain_relevant_docs(queries, bm25, lowercase = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3zJSAwX9Vgj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fcc4ea8-1b17-4065-95c7-6b5ee989ab31"
      },
      "source": [
        "map"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4069061323357765"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z0nR-Ba9nuH"
      },
      "source": [
        "list_tokens, indexes_of_doc = process_corpus(\"FIRE2017-IRLeD-track-data/Task_2/Prior_Cases\", lowercase = True)\n",
        "MAP = tune_hyperparameters(lowercase = True, model = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8xwzCdmAPzG",
        "outputId": "1b3640f3-1ed1-476e-d4ea-e459cd843867"
      },
      "source": [
        "list_tokens, indexes_of_doc = process_corpus(\"FIRE2017-IRLeD-track-data/Task_2/Prior_Cases\", lowercase = True)\n",
        "MAP = tune_hyperparameters(lowercase = True)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4069061323357765"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4_5nItLBPx1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}